\documentclass[12pt]{article}
\usepackage{graphicx}

\title{Statistical programming \\ Assignment 2  \\ (\texttt{Group 1})}
\author{Andreas Hild, Andreas Pettersson, Henrik Bark}
\date{\today}
\renewcommand*\contentsname{Table of contents}
\begin{document}
\maketitle 
\tableofcontents
\newpage
\section{Task 1: Ridge regression}

<<include = FALSE>>=
opts_chunk$set(size = "footnotesize",
comment = NA,
background = "#E7E7E7",
prompt = FALSE,
fig.width = 6,
fig.asp = 0.62,
fig.align = "center")
@

<<include=FALSE>>=
library(dplyr)
library(tidyverse)
library(ggplot2)
library(devtools)
library(modelr)

@

\maketitle 
\section{Section 2}
\subsection{Task 1 - Loading the data}

The biopsy data is loaded in our global enviroment. 

<<include = FALSE>>=
### Assigment 2 

library(stringi)
library(tidyverse)
library(MLmetrics)
library(reshape2)
@

<<>>=

# set seed to one for reproducibility

set.seed(1)

biopsy_data = function(loc = "biopsy.csv"){
  biop = na.omit(read.csv(loc, na.strings = c('?', "NA")))
  train_ids = sample(nrow(biop), size = 300)
  X_train = biop[train_ids, names(biop) %in% stri_paste("V", 1:9)]
  x_test = biop[-train_ids, names(biop) %in% stri_paste("V", 1:9)]
  y_train = biop[train_ids, names(biop) == "class"]
  y_test = biop[-train_ids, names(biop) == "class"]
  return(list(X_train = as.matrix(X_train),
              x_test = as.matrix(x_test),
              y_train = as.matrix(as.integer(y_train)),
              y_test = as.matrix(as.integer(y_test))))
}

bio <- biopsy_data() 

## Changing the structure of 2 and 1 to 1 and 0 for the dummy variables

bio$intercept <- matrix(1, nrow(bio$X_train), 1)
bio$y_train <- ifelse(bio$y_train == 2, 1, 0)
bio$y_test <- ifelse(bio$y_test == 2, 1, 0)

## Checking the structure of the imported data

str(bio)
@

\subsection{Task 2 - Implementing equation 9 and 10}

Implementing eq 9 and 10
<<>>==

z = function(X, beta, beta0){
  
  linear_transformation = beta0 + X %*% beta  
  
  return(linear_transformation)
}


sigmoid = function(z){
  sigmoid_transformation = 1/ (1 + exp(-z))
  return(sigmoid_transformation)
}

cross_entropy = function(sigmoid, y){
  ## Initiate a starting value
  loss = 0
  
  for(i in 1:length(sigmoid)){
    loss = loss + ( (- y[i]) * log(sigmoid[i])
  - ((1 - y[i]) * log(1 - sigmoid[i]) )) / length(sigmoid)   
    
  }
  
  return(loss)
}
@

Ensuring that everything works with a simple toy example
<<>>==
X = matrix(c(1,0,3,1,1,5), ncol = 3)
X = cbind(1, X)
y = c(0, 1)
b0 = 0.5
b = c(b0, 2.5, -1, 2)
lin = z(X, b, 0) # setting a zero for b0 since it is already in the b vector
h = sigmoid(lin)
cross_entropy(h, y)
@

\subsection{Task 3 - Estimating the intercept and the coefficients}

Estimating the intercept and regression coefficients by gradient descent 
<<>>==

# the zero in the Z function is in order to have a 
# general formula which both beta and beta0 into account

gradient_descent = function(X, y, epochs, stepsize){
  
  beta <-  matrix(0, ncol(bio$X_train), 1)
  beta0 <- 0
  loss = rep(0,epochs)
  
for(i in 1:epochs){
  beta0 = beta0 - stepsize * (1 / length(y) * sum(sigmoid(z(X, beta, beta0)) - y))
  beta = beta - stepsize * (1 / length(y) * t(X) %*% (sigmoid(z(X, beta, beta0)) - y))
  

    
    loss[i] = cross_entropy(sigmoid(z(X, beta, beta0)), y)
  }
  
  
  return(list(beta = beta,
              intercept = beta0, loss = loss))
}
@

\subsection{Task 4 - Loss of epochs set to 500}

Setting epochs to 500 and comparing the loss for stepsize .001, .01 and .05 

<<warning = FALSE>>==
step.001 <- gradient_descent(bio$X_train, bio$y_train, 500, 0.001)
step.01 <- gradient_descent(bio$X_train, bio$y_train, 500, 0.01)
step.05 <- gradient_descent(bio$X_train, bio$y_train, 500, 0.05)

dataplot <- as.tibble(list(step.001 = step.001$loss, 
                           step.01= step.01$loss, 
                           step.05 = step.05$loss,
                           id = rep(1:500)))

dataplot = melt(dataplot, id = "id")



ggplot(dataplot, aes(x = id)) +
  geom_line(aes(y = value, color = variable), size = 1.2) +
  labs(y = "Cross entropy loss",
       x = "Epoch",
       title = "Cross entropy loss for stepsizes 0.001, 0.01 and 0.05", 
       colour = "Stepsize") +
  scale_color_manual(values = c("darkgreen", 
  "darkblue", "darkturquoise"), labels = c("0.001", "0.01", "0.05"))
@

By looking at the plot of the calculated loss at every iteration. It's clear that 0.05 yields the lowest loss and a stepsize of 0.001 has the biggest loss. The is most likely because the steps are too small when the stepsizes are 0.001 and 0.01 which in theory means that the parameters are not yet optimized. Therefore, we would choose a stepsize of 0.05 if we were to do 500 iterations given our data and with taking the randomness of the data sampling into account. 

\subsection{Task 5 - Prediction function}

Write a prediction function
<<>>==
## Prediction function 

predict_logistic = function(X, y, beta, beta0, p_cutoff = .5){
  
  ## create a matrix where all values will be stored
  probabilities = matrix(0, nrow(X), 2)
  
  probabilities[,1] = sigmoid(z(X, beta, beta0))
  
  ## Loop for binary classification, if the predicted probability is greater than
  ## the cutoff then that observations will be predicted to be classified as a one. 
  for(i in 1:nrow(X)){
    if(probabilities[i,1] > p_cutoff ){
      probabilities[i,2] = 1
    } else{probabilities[i,2] = 0}
  }
  
  predictions = sum(probabilities[,2] == y ) / nrow(X)
  
  return(predictions)
  
}

step.001 <- gradient_descent(bio$X_train, bio$y_train, 500, 0.001)
step.01 <- gradient_descent(bio$X_train, bio$y_train, 500, 0.01)
step.05 <- gradient_descent(bio$X_train, bio$y_train, 500, 0.05)

prediction_list <- list(stepsize.001 = (predict_logistic(bio$x_test, bio$y_test, 
                        step.001$beta, step.001$intercept)),
                        stepsize.01 = (predict_logistic(bio$x_test, bio$y_test, 
                        step.01$beta, step.01$intercept)),
                        stepsize.05 = (predict_logistic(bio$x_test, bio$y_test, 
                        step.05$beta, step.05$intercept)))


prediction_list
@

By running the predictiong function in order to check the prediction accuracy, the stepsize of 0.05 still performs best.
\subsection{Task 6 - Setting epochs to 2000}

<<>>==
# Setting the epochs to 2000 

step.001 <- gradient_descent(bio$X_train, bio$y_train, 2000, 0.001)
step.01 <- gradient_descent(bio$X_train, bio$y_train, 2000, 0.01)
step.05 <- gradient_descent(bio$X_train, bio$y_train, 2000, 0.05)


prediction_list <- list(stepsize.001 = (predict_logistic(bio$x_test, 
                        bio$y_test, step.001$beta, step.001$intercept)),
                        stepsize.01 = (predict_logistic(bio$x_test, 
                        bio$y_test, step.01$beta, step.01$intercept)),
                        stepsize.05 = (predict_logistic(bio$x_test, 
                        bio$y_test, step.05$beta, step.05$intercept)))

prediction_list
@

When the epochs are set to 2000, the prediction accuracy goes up for all procedures. The model still performs best with a stepsize to 05. 

\end{document}
